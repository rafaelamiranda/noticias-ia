<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Notícias de Inteligência Artificial (EN)</title>
    <link>https://github.com/rafaelamiranda/noticias-ia</link>
    <description>
Principais notícias relacionadas a inteligência artificial
dos últimos 7 dias, em Inglês.
    </description>
    <lastBuildDate>Sat, 09 Aug 2025 09:04:40 GMT</lastBuildDate>
    <item>
      <title>Truth Social’s New AI Chatbot Is Donald Trump’s Media Diet Incarnate</title>
      <link>https://www.wired.com/story/i-fear-truth-search-ai-might-be-biased-but-it-says-it-isnt/</link>
      <description>Save StorySave this story</description>
      <pubDate>Fri, 08 Aug 2025 22:20:48 GMT</pubDate>
      <guid>https://www.wired.com/story/i-fear-truth-search-ai-might-be-biased-but-it-says-it-isnt/</guid>
    </item>
    <item>
      <title>The Vibes-Based Pricing of ‘Pro’ AI Software</title>
      <link>https://www.wired.com/story/uncanny-valley-podcast-vibes-based-pricing-pro-ai-software/</link>
      <description>In this episode of Uncanny Valley, we&#x27;re talking about why some chatbot subscriptions are so expensive and how these premium prices were determined on vibes more than anything substantial.</description>
      <pubDate>Fri, 08 Aug 2025 19:46:11 GMT</pubDate>
      <guid>https://www.wired.com/story/uncanny-valley-podcast-vibes-based-pricing-pro-ai-software/</guid>
    </item>
    <item>
      <title>Join Our Next Livestream: What GPT-5 Means for ChatGPT Users</title>
      <link>https://www.wired.com/story/what-gpt-5-means-for-chatgpt-users/</link>
      <description>On August 14, bring your questions about OpenAI&#x27;s latest model, GPT-5, and what it means for the future of chatbots.</description>
      <pubDate>Fri, 08 Aug 2025 18:25:10 GMT</pubDate>
      <guid>https://www.wired.com/story/what-gpt-5-means-for-chatgpt-users/</guid>
    </item>
    <item>
      <title>Suvianna Grecu, AI for Change: Without rules, AI risks ‘trust crisis’</title>
      <link>https://www.artificialintelligence-news.com/news/suvianna-grecu-ai-for-change-without-rules-ai-risks-trust-crisis/</link>
      <description>The world is in a race to deploy AI, but a leading voice in technology ethics warns prioritising speed over safety risks a “trust crisis.”Suvianna Grecu, Founder of the AI for Change Foundation, argues that without immediate and strong governance, we are on a path to “automating harm at scale.”Speaking on the integration of AI into critical sectors, Grecu believes that the most pressing ethical danger isn’t the technology itself, but the lack of structure surrounding its rollout.Powerful systems are increasingly making life-altering decisions about everything from job applications and credit scores to healthcare and criminal justice, often without sufficient testing for bias or consideration of their long-term societal impact.For many organisations, AI ethics remains a document of lofty principles rather than a daily operational reality. Grecu insists that genuine accountability only begins when someone is made truly responsible for the outcomes. The gap between intention and implementation is where the real risk lies.Grecu’s foundation champions a shift from abstract ideas to concrete action. This involves embedding ethical considerations directly into development workflows through practical tools like design checklists, mandatory pre-deployment risk assessments, and cross-functional review boards that bring legal, technical, and policy teams together.According to Grecu, the key is establishing clear ownership at every stage, building transparent and repeatable processes just as you would for any other core business function. This practical approach seeks to advance ethical AI, transforming it from a philosophical debate into a set of manageable, everyday tasks.Partnering to build AI trust and mitigate risksWhen it comes to enforcement, Grecu is clear that the responsibility can’t fall solely on government or industry. “It’s not either-or, it has to be both,” she states, advocating for a collaborative model.In this partnership, governments must set the legal boundaries and minimum standards, particularly where fundamental human rights are at stake. Regulation provides the essential floor. However, industry possesses the agility and technical talent to innovate beyond mere compliance.Companies are best positioned to create advanced auditing tools, pioneer n...</description>
      <pubDate>Fri, 08 Aug 2025 16:04:09 GMT</pubDate>
      <guid>https://www.artificialintelligence-news.com/news/suvianna-grecu-ai-for-change-without-rules-ai-risks-trust-crisis/</guid>
    </item>
    <item>
      <title>OpenAI Finally Launched GPT-5. Here&#x27;s Everything You Need to Know</title>
      <link>https://www.wired.com/story/openais-gpt-5-is-here/</link>
      <description>Save StorySave this story</description>
      <pubDate>Thu, 07 Aug 2025 17:00:33 GMT</pubDate>
      <guid>https://www.wired.com/story/openais-gpt-5-is-here/</guid>
    </item>
    <item>
      <title>Age Verification Is Sweeping Gaming. Is It Ready for the Age of AI Fakes?</title>
      <link>https://www.wired.com/story/age-verification-is-sweeping-gaming-is-it-ready-for-the-age-of-ai-fakes/</link>
      <description>Discord users are already using video game characters to bypass the UK’s age-check laws. AI deepfakes could make things even more complicated.</description>
      <pubDate>Thu, 07 Aug 2025 16:30:00 GMT</pubDate>
      <guid>https://www.wired.com/story/age-verification-is-sweeping-gaming-is-it-ready-for-the-age-of-ai-fakes/</guid>
    </item>
    <item>
      <title>Alan Turing Institute: Humanities are key to the future of AI</title>
      <link>https://www.artificialintelligence-news.com/news/alan-turing-institute-humanities-are-key-future-of-ai/</link>
      <description>A powerhouse team has launched a new initiative called ‘Doing AI Differently,’ which calls for a human-centred approach to future development.For years, we’ve treated AI’s outputs like they’re the results of a giant math problem. But the researchers – from The Alan Turing Institute, the University of Edinburgh, AHRC-UKRI, and the Lloyd’s Register Foundation – behind this project say that’s the wrong way to look at it.What AI is creating are basically cultural artifacts. They’re more like a novel or a painting than a spreadsheet. The problem is, AI is creating this “culture” without understanding any of it. It’s like someone who has memorised a dictionary but has no idea how to hold a real conversation.This is why AI often fails when “nuance and context matter most,” says Professor Drew Hemment, Theme Lead for Interpretive Technologies for Sustainability at The Alan Turing Institute. The system just doesn’t have the “interpretive depth” to get what it’s really saying.However, most of the AI in the world is built on just a handful of similar designs. The report calls this the “homogenisation problem” and future AI development must overcome this.Imagine if every baker in the world used the exact same recipe. You’d get a lot of identical, and frankly, boring cakes. With AI, this means the same blind spots, the same biases, and the same limitations get copied and pasted into thousands of tools we use every day.We saw this happen with social media. It was rolled out with simple goals, and we’re now living with the unintended societal consequences. The ‘Doing AI Differently’ team is sounding the alarm to make sure we don’t make that same mistake with AI.The team has a plan to build a new kind of AI, one they call Interpretive AI. It’s about designing systems from the very beginning to work the way people do; with ambiguity, multiple viewpoints, and a deep understanding of context.The vision is to create interpretive technologies that can offer multiple valid perspectives instead of just one rigid answer. It also means exploring alternative AI architectures to break the mould of current designs. Most importantly, the future isn’t about AI replacing us; it’s about creating human-AI ensembles where we work together, combining our creativity with AI’s processing power...</description>
      <pubDate>Thu, 07 Aug 2025 15:18:27 GMT</pubDate>
      <guid>https://www.artificialintelligence-news.com/news/alan-turing-institute-humanities-are-key-future-of-ai/</guid>
    </item>
    <item>
      <title>A Single Poisoned Document Could Leak ‘Secret’ Data Via ChatGPT</title>
      <link>https://www.wired.com/story/poisoned-document-could-leak-secret-data-chatgpt/</link>
      <description>Save StorySave this story</description>
      <pubDate>Wed, 06 Aug 2025 23:30:00 GMT</pubDate>
      <guid>https://www.wired.com/story/poisoned-document-could-leak-secret-data-chatgpt/</guid>
    </item>
    <item>
      <title>Inside the US Government&#x27;s Unpublished Report on AI Safety</title>
      <link>https://www.wired.com/story/inside-the-biden-administrations-unpublished-report-on-ai-safety/</link>
      <description>The National Institute of Standards and Technology conducted a groundbreaking study on frontier models just before Donald Trump&#x27;s second term as president—and never published the results.</description>
      <pubDate>Wed, 06 Aug 2025 18:00:00 GMT</pubDate>
      <guid>https://www.wired.com/story/inside-the-biden-administrations-unpublished-report-on-ai-safety/</guid>
    </item>
    <item>
      <title>OpenAI Announces Massive US Government Partnership</title>
      <link>https://www.wired.com/story/openai-is-giving-chatgpt-federal-workers/</link>
      <description>Save StorySave this story</description>
      <pubDate>Wed, 06 Aug 2025 16:48:11 GMT</pubDate>
      <guid>https://www.wired.com/story/openai-is-giving-chatgpt-federal-workers/</guid>
    </item>
    <item>
      <title>AI obsession is costing us our human skills</title>
      <link>https://www.artificialintelligence-news.com/news/ai-obsession-costing-us-our-human-skills/</link>
      <description>A growing body of evidence suggests that over-reliance on AI could be eroding the human skills needed to use it effectively. Research warns this emerging human skills deficit threatens the successful adoption of AI and, with it, an opportunity for economic growth. It feels like not a day goes by without another proclamation about how […]
The post AI obsession is costing us our human skills appeared first on AI News.</description>
      <pubDate>Wed, 06 Aug 2025 15:48:14 GMT</pubDate>
      <guid>https://www.artificialintelligence-news.com/news/ai-obsession-costing-us-our-human-skills/</guid>
    </item>
    <item>
      <title>Generative AI trends 2025: LLMs, data scaling &amp; enterprise adoption</title>
      <link>https://www.artificialintelligence-news.com/news/generative-ai-trends-2025-llms-data-scaling-enterprise-adoption/</link>
      <description>Generative AI is entering a more mature phase in 2025. Models are being refined for accuracy and efficiency, and enterprises are embedding them into everyday workflows.The focus is shifting from what these systems could do to how they can be applied reliably and at scale. What’s emerging is a clearer picture of what it takes to build generative AI that is not just powerful, but dependable.The new generation of LLMsLarge language models are shedding their reputation as resource-hungry giants. The cost of generating a response from a model has dropped by a factor of 1,000 over the past two years, bringing it in line with the cost of a basic web search. That shift is making real-time AI far more viable for routine business tasks.Scale with control is also this year’s priority. The leading models (Claude Sonnet 4, Gemini Flash 2.5, Grok 4, DeepSeek V3) are still large, but they’re built to respond faster, reason more clearly, and run more efficiently. Size alone is no longer the differentiator. What matters is whether a model can handle complex input, support integration, and deliver reliable outputs, even when complexity increases. Last year saw a lot of criticism of AI’s tendency to hallucinate. In one high-profile case, a New York lawyer faced sanctions for citing ChatGPT-invented legal cases. Similar failures across sensitive sectors pushed the issue into the spotlight.This is something LLM companies have been combating this year. Retrieval-augmented generation (RAG), which combines search with generation to ground outputs in real data, has become a common approach. It helps reduce hallucinations but not eliminate them. Models can still contradict the retrieved content. New benchmarks such as RGB and RAGTruth are being used to track and quantify these failures, marking a shift toward treating hallucination as a measurable engineering problem rather than an acceptable flaw.Navigating rapid innovationOne of the defining trends of 2025 is the speed of change. Model releases are accelerating, capabilities are shifting monthly, and what counts as state-of-the-art is constantly being redefined. For enterprise leaders, this creates a knowledge gap that can quickly turn into a competitive one.Staying ahead means staying informed. Events like the AI and Big Data Exp...</description>
      <pubDate>Wed, 06 Aug 2025 15:02:37 GMT</pubDate>
      <guid>https://www.artificialintelligence-news.com/news/generative-ai-trends-2025-llms-data-scaling-enterprise-adoption/</guid>
    </item>
    <item>
      <title>These Democrats Think the Party Needs AI to Win Elections</title>
      <link>https://www.wired.com/story/democrats-midterm-elections-ai/</link>
      <description>Save StorySave this story</description>
      <pubDate>Wed, 06 Aug 2025 14:52:56 GMT</pubDate>
      <guid>https://www.wired.com/story/democrats-midterm-elections-ai/</guid>
    </item>
    <item>
      <title>Hackers Hijacked Google’s Gemini AI With a Poisoned Calendar Invite to Take Over a Smart Home</title>
      <link>https://www.wired.com/story/google-gemini-calendar-invite-hijack-smart-home/</link>
      <description>Save StorySave this story</description>
      <pubDate>Wed, 06 Aug 2025 13:00:00 GMT</pubDate>
      <guid>https://www.wired.com/story/google-gemini-calendar-invite-hijack-smart-home/</guid>
    </item>
    <item>
      <title>Inside Tim Cook’s push to get Apple back in the AI race</title>
      <link>https://www.artificialintelligence-news.com/news/inside-tim-cook-push-to-get-apple-back-in-the-ai-race/</link>
      <description>While other tech companies push out AI tools at full speed, Apple is taking its time. Its Apple Intelligence features – shown off at WWDC – won’t reach most users until at least 2025 or even 2026. Some see this as Apple falling behind, but the company’s track record suggests it prefers to launch only […]
The post Inside Tim Cook’s push to get Apple back in the AI race appeared first on AI News.</description>
      <pubDate>Wed, 06 Aug 2025 09:21:51 GMT</pubDate>
      <guid>https://www.artificialintelligence-news.com/news/inside-tim-cook-push-to-get-apple-back-in-the-ai-race/</guid>
    </item>
    <item>
      <title>Tencent releases versatile open-source Hunyuan AI models</title>
      <link>https://www.artificialintelligence-news.com/news/tencent-releases-versatile-open-source-hunyuan-ai-models/</link>
      <description>Tencent has expanded its family of open-source Hunyuan AI models that are versatile enough for broad use. This new family of models is engineered to deliver powerful performance across computational environments, from small edge devices to demanding, high-concurrency production systems. The release includes a comprehensive set of pre-trained and instruction-tuned models available on the developer […]
The post Tencent releases versatile open-source Hunyuan AI models appeared first on AI News.</description>
      <pubDate>Mon, 04 Aug 2025 14:58:20 GMT</pubDate>
      <guid>https://www.artificialintelligence-news.com/news/tencent-releases-versatile-open-source-hunyuan-ai-models/</guid>
    </item>
  </channel>
</rss>